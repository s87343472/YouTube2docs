import fs from 'fs/promises'
import path from 'path'
import { initGroq, initTogether, hasGroqKey, hasTogetherKey, API_CONFIG } from '../config/apis'
import { TranscriptionResult, TranscriptionSegment, AudioExtractionResult } from '../types'
import { AudioProcessor } from './audioProcessor'
import { ApiQuotaMonitor } from './apiQuotaMonitor'

/**
 * éŸ³é¢‘è½¬å½•æœåŠ¡ç±»
 * æ”¯æŒGroqå’ŒTogether AIçš„Whisperæ¨¡å‹ï¼Œè‡ªåŠ¨é™çº§åˆ‡æ¢
 */
export class TranscriptionService {
  private static readonly RETRY_ATTEMPTS = 3
  private static readonly RETRY_DELAY = 1000 // 1ç§’
  private static readonly RATE_LIMIT_RETRY_DELAY = 5 * 60 * 1000 // 5åˆ†é’Ÿ
  private static readonly MAX_RATE_LIMIT_RETRIES = 1 // æœ€å¤šç­‰å¾…ä¸€æ¬¡é™æµæ¢å¤

  /**
   * è½¬å½•éŸ³é¢‘æ–‡ä»¶ - æ™ºèƒ½å¤šæä¾›å•†é™çº§
   */
  static async transcribeAudio(
    audioPath: string, 
    language?: string
  ): Promise<TranscriptionResult> {
    // æ£€æŸ¥å¯ç”¨çš„APIæä¾›å•†
    const hasGroq = hasGroqKey()
    const hasTogether = hasTogetherKey()
    
    if (!hasGroq && !hasTogether) {
      throw new Error('æ²¡æœ‰å¯ç”¨çš„è½¬å½•APIå¯†é’¥ã€‚è¯·é…ç½®Groqæˆ–Together AIå¯†é’¥ã€‚')
    }

    console.log(`ğŸ¤ Starting transcription: ${audioPath}`)
    console.log(`ğŸ“¡ Available providers: ${hasGroq ? 'Groq' : ''}${hasGroq && hasTogether ? ', ' : ''}${hasTogether ? 'Together' : ''}`)
    
    // éªŒè¯éŸ³é¢‘æ–‡ä»¶
    await this.validateAudioFile(audioPath)
    
    let lastError: Error | null = null
    
    // ä¼˜å…ˆå°è¯•Groqï¼ˆæ›´å¿«é€Ÿï¼‰
    if (hasGroq) {
      try {
        console.log('ğŸš€ Attempting transcription with Groq...')
        const result = await this.transcribeWithGroq(audioPath, language)
        console.log(`âœ… Groq transcription completed: ${result.text.length} characters`)
        return result
      } catch (error) {
        lastError = error as Error
        console.warn('âš ï¸ Groq transcription failed:', error)
        
        // å¦‚æœæ˜¯é™æµé”™è¯¯ä¸”æœ‰Togetherå¤‡ç”¨ï¼Œç»§ç»­å°è¯•
        if (hasTogether && this.shouldFallbackToTogether(error)) {
          console.log('ğŸ”„ Falling back to Together AI...')
        } else {
          // å¦‚æœä¸æ˜¯é™æµé”™è¯¯æˆ–æ²¡æœ‰å¤‡ç”¨æ–¹æ¡ˆï¼Œç›´æ¥æŠ›å‡ºé”™è¯¯
          throw error
        }
      }
    }
    
    // å°è¯•Together AI
    if (hasTogether) {
      try {
        console.log('ğŸŒŸ Attempting transcription with Together AI...')
        const result = await this.transcribeWithTogether(audioPath, language)
        console.log(`âœ… Together AI transcription completed: ${result.text.length} characters`)
        return result
      } catch (error) {
        lastError = error as Error
        console.error('âŒ Together AI transcription failed:', error)
      }
    }
    
    // æ‰€æœ‰æä¾›å•†éƒ½å¤±è´¥äº†
    throw lastError || new Error('æ‰€æœ‰è½¬å½•æœåŠ¡éƒ½ä¸å¯ç”¨')
  }

  /**
   * ä½¿ç”¨Groqè¿›è¡Œè½¬å½•
   */
  private static async transcribeWithGroq(
    audioPath: string,
    language?: string
  ): Promise<TranscriptionResult> {
    // è·å–éŸ³é¢‘æ—¶é•¿å¹¶æ£€æŸ¥é…é¢
    const audioDuration = await AudioProcessor.getAudioDuration(audioPath)
    const quotaCheck = await ApiQuotaMonitor.canProcessAudio(audioDuration)
    
    if (!quotaCheck.canProcess) {
      throw new Error(`Groq APIé…é¢ä¸è¶³ï¼š${quotaCheck.reason}`)
    }
    
    console.log(`ğŸ“Š Groq quota check passed: ${quotaCheck.currentUsage}/${quotaCheck.limit}s used`)
    
    const groq = initGroq()
    
    // è¯»å–éŸ³é¢‘æ–‡ä»¶
    const audioBuffer = await fs.readFile(audioPath)
    const audioFile = new File([audioBuffer], path.basename(audioPath), {
      type: this.getMimeType(audioPath)
    })

    let lastError: Error | null = null

    // é‡è¯•æœºåˆ¶
    for (let attempt = 1; attempt <= this.RETRY_ATTEMPTS; attempt++) {
      try {
        console.log(`ğŸ”„ Groq attempt ${attempt}/${this.RETRY_ATTEMPTS}`)
        
        const transcription = await groq.audio.transcriptions.create({
          file: audioFile,
          model: API_CONFIG.GROQ.MODEL,
          language: language || 'zh',
          response_format: 'verbose_json',
          temperature: 0.0
        })

        // è®°å½•APIä½¿ç”¨æƒ…å†µ
        await ApiQuotaMonitor.recordApiUsage('groq', 'transcription', audioDuration, 0, audioDuration * 0.0002)
        
        return this.processTranscriptionResult(transcription)

      } catch (error) {
        lastError = error as Error
        console.error(`âŒ Groq attempt ${attempt} failed:`, error)
        
        // æ£€æŸ¥æ˜¯å¦æ˜¯é™æµé”™è¯¯
        if (this.isRateLimitError(error)) {
          const retryAfter = this.extractRetryAfter(error)
          console.warn(`â³ Groq rate limit: wait ${Math.ceil(retryAfter / 60)}m${Math.ceil((retryAfter % 60))}s`)
          
          // å¦‚æœé‡è¯•æ—¶é—´è¶…è¿‡5åˆ†é’Ÿï¼Œç›´æ¥æŠ›å‡ºé™æµé”™è¯¯ä¾›ä¸Šå±‚é™çº§
          if (retryAfter > this.RATE_LIMIT_RETRY_DELAY / 1000) {
            throw new Error(`Groq APIé™æµï¼šéœ€è¦ç­‰å¾…${Math.ceil(retryAfter / 60)}åˆ†é’Ÿ`)
          }
          
          // ç­‰å¾…é™æµæ¢å¤ï¼ˆä»…ç¬¬ä¸€æ¬¡å°è¯•ï¼‰
          if (attempt === 1) {
            console.log(`â³ Waiting ${Math.ceil(retryAfter)}s for Groq rate limit to reset...`)
            await this.delay(retryAfter * 1000)
            continue
          } else {
            throw new Error(`Groq APIé™æµï¼šå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°`)
          }
        }
        
        if (attempt < this.RETRY_ATTEMPTS) {
          await this.delay(this.RETRY_DELAY * attempt)
        }
      }
    }

    throw lastError || new Error('Groq transcription failed after all retries')
  }

  /**
   * ä½¿ç”¨Together AIè¿›è¡Œè½¬å½•
   */
  private static async transcribeWithTogether(
    audioPath: string,
    language?: string
  ): Promise<TranscriptionResult> {
    const together = initTogether()
    
    // è¯»å–éŸ³é¢‘æ–‡ä»¶
    const audioBuffer = await fs.readFile(audioPath)
    const audioFile = new File([audioBuffer], path.basename(audioPath), {
      type: this.getMimeType(audioPath)
    })

    let lastError: Error | null = null

    // é‡è¯•æœºåˆ¶
    for (let attempt = 1; attempt <= this.RETRY_ATTEMPTS; attempt++) {
      try {
        console.log(`ğŸ”„ Together AI attempt ${attempt}/${this.RETRY_ATTEMPTS}`)
        
        const transcription = await together.audio.transcriptions.create({
          file: audioFile,
          model: API_CONFIG.TOGETHER.MODEL as "openai/whisper-large-v3",
          language: language || 'zh',
          response_format: 'json',
          timestamp_granularities: 'segment' as "segment"
        })

        // è®°å½•APIä½¿ç”¨æƒ…å†µ
        const audioDuration = await AudioProcessor.getAudioDuration(audioPath)
        await ApiQuotaMonitor.recordApiUsage('together', 'transcription', audioDuration, 0, audioDuration * 0.0001)
        
        return this.processTogetherTranscriptionResult(transcription)

      } catch (error) {
        lastError = error as Error
        console.error(`âŒ Together AI attempt ${attempt} failed:`, error)
        
        if (attempt < this.RETRY_ATTEMPTS) {
          await this.delay(this.RETRY_DELAY * attempt)
        }
      }
    }

    throw lastError || new Error('Together AI transcription failed after all retries')
  }

  /**
   * å¤„ç†è½¬å½•ç»“æœ
   */
  private static processTranscriptionResult(transcription: any): TranscriptionResult {
    const result: TranscriptionResult = {
      text: transcription.text || '',
      language: transcription.language || 'unknown',
      confidence: 0.95, // Groq Whisperé€šå¸¸æœ‰å¾ˆé«˜çš„å‡†ç¡®æ€§
      segments: []
    }

    // å¤„ç†åˆ†æ®µä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if (transcription.segments && Array.isArray(transcription.segments)) {
      result.segments = transcription.segments.map((segment: any) => ({
        start: segment.start || 0,
        end: segment.end || 0,
        text: segment.text || '',
        confidence: segment.avg_logprob ? Math.exp(segment.avg_logprob) : 0.9
      }))
    }

    return result
  }

  /**
   * æ‰¹é‡è½¬å½•å¤šä¸ªéŸ³é¢‘æ–‡ä»¶
   */
  static async transcribeMultipleFiles(
    audioPaths: string[],
    language?: string
  ): Promise<TranscriptionResult> {
    const results: TranscriptionResult[] = []
    
    console.log(`ğŸµ Transcribing ${audioPaths.length} audio segments`)

    for (let i = 0; i < audioPaths.length; i++) {
      const audioPath = audioPaths[i]
      console.log(`ğŸ“ Processing segment ${i + 1}/${audioPaths.length}`)
      
      try {
        const result = await this.transcribeAudio(audioPath, language)
        results.push(result)
      } catch (error) {
        console.error(`âŒ Failed to transcribe segment ${i + 1}:`, error)
        // ç»§ç»­å¤„ç†å…¶ä»–æ®µï¼Œä½†è®°å½•é”™è¯¯
        results.push({
          text: `[Transcription failed for segment ${i + 1}]`,
          language: language || 'unknown',
          confidence: 0,
          segments: []
        })
      }
    }

    // åˆå¹¶æ‰€æœ‰è½¬å½•ç»“æœ
    return this.mergeTranscriptionResults(results)
  }

  /**
   * åˆå¹¶å¤šä¸ªè½¬å½•ç»“æœ
   */
  private static mergeTranscriptionResults(results: TranscriptionResult[]): TranscriptionResult {
    const mergedResult: TranscriptionResult = {
      text: '',
      language: results[0]?.language || 'unknown',
      confidence: 0,
      segments: []
    }

    let totalConfidence = 0
    let segmentOffset = 0

    for (const result of results) {
      // åˆå¹¶æ–‡æœ¬
      if (mergedResult.text && result.text) {
        mergedResult.text += ' '
      }
      mergedResult.text += result.text

      // ç´¯è®¡ç½®ä¿¡åº¦
      totalConfidence += result.confidence

      // åˆå¹¶åˆ†æ®µï¼ˆè°ƒæ•´æ—¶é—´åç§»ï¼‰
      if (result.segments) {
        const adjustedSegments = result.segments.map(segment => ({
          ...segment,
          start: segment.start + segmentOffset,
          end: segment.end + segmentOffset
        }))
        mergedResult.segments!.push(...adjustedSegments)
        
        // æ›´æ–°åç§»é‡ï¼ˆå‡è®¾æ¯æ®µå¤§çº¦10åˆ†é’Ÿï¼‰
        segmentOffset += 600
      }
    }

    // è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
    mergedResult.confidence = results.length > 0 ? totalConfidence / results.length : 0

    return mergedResult
  }

  /**
   * æ™ºèƒ½è½¬å½•ï¼ˆåŒ…å«éŸ³é¢‘é¢„å¤„ç†å’Œåˆ†å‰²ï¼‰
   */
  static async smartTranscribe(
    audioResult: AudioExtractionResult,
    videoId: string,
    language?: string
  ): Promise<TranscriptionResult> {
    console.log(`ğŸ§  Starting smart transcription for ${videoId}`)

    try {
      // æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œå†³å®šæ˜¯å¦éœ€è¦åˆ†å‰²
      if (audioResult.size > API_CONFIG.GROQ.MAX_FILE_SIZE) {
        console.log('ğŸ“‚ Audio file too large, splitting into segments')
        
        // ä½¿ç”¨AudioProcessoråˆ†å‰²å¤§æ–‡ä»¶
        const segmentPaths = await AudioProcessor.splitLargeAudio(audioResult.audioPath, videoId)
        
        if (segmentPaths.length === 0) {
          throw new Error('Failed to split audio file into segments')
        }
        
        console.log(`ğŸ“ Split audio into ${segmentPaths.length} segments`)
        return await this.transcribeMultipleFiles(segmentPaths, language)
      } else {
        return await this.transcribeAudio(audioResult.audioPath, language)
      }

    } catch (error) {
      console.error('âŒ Smart transcription failed:', error)
      throw error
    }
  }

  /**
   * éªŒè¯éŸ³é¢‘æ–‡ä»¶
   */
  private static async validateAudioFile(audioPath: string): Promise<void> {
    try {
      await fs.access(audioPath)
      
      const stats = await fs.stat(audioPath)
      if (stats.size === 0) {
        throw new Error('Audio file is empty')
      }
      
      if (stats.size > API_CONFIG.GROQ.MAX_FILE_SIZE) {
        throw new Error('Audio file exceeds maximum size limit')
      }

      const extension = path.extname(audioPath).toLowerCase().slice(1)
      if (!API_CONFIG.GROQ.SUPPORTED_FORMATS.includes(extension)) {
        throw new Error(`Audio format '${extension}' is not supported`)
      }

    } catch (error) {
      throw new Error(`Audio file validation failed: ${error instanceof Error ? error.message : String(error)}`)
    }
  }

  /**
   * è·å–éŸ³é¢‘æ–‡ä»¶çš„MIMEç±»å‹
   */
  private static getMimeType(audioPath: string): string {
    const extension = path.extname(audioPath).toLowerCase()
    const mimeTypes: { [key: string]: string } = {
      '.mp3': 'audio/mpeg',
      '.wav': 'audio/wav',
      '.m4a': 'audio/mp4',
      '.webm': 'audio/webm',
      '.mp4': 'video/mp4'
    }
    
    return mimeTypes[extension] || 'audio/mpeg'
  }

  /**
   * å»¶è¿Ÿå‡½æ•°
   */
  private static delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms))
  }

  /**
   * æ£€æŸ¥æ˜¯å¦æ˜¯é™æµé”™è¯¯
   */
  private static isRateLimitError(error: any): boolean {
    return error?.status === 429 || 
           error?.error?.code === 'rate_limit_exceeded' ||
           (error?.message && error.message.includes('Rate limit'))
  }

  /**
   * ä»é”™è¯¯ä¿¡æ¯ä¸­æå–é‡è¯•ç­‰å¾…æ—¶é—´
   */
  private static extractRetryAfter(error: any): number {
    // ä¼˜å…ˆä»headersä¸­è·å–retry-after
    if (error?.headers?.['retry-after']) {
      return parseInt(error.headers['retry-after'], 10)
    }
    
    // ä»é”™è¯¯æ¶ˆæ¯ä¸­è§£ææ—¶é—´
    const message = error?.error?.message || error?.message || ''
    
    // åŒ¹é… "Please try again in 19m14.152s" æ ¼å¼
    const timeMatch = message.match(/try again in (\d+)m(\d+(?:\.\d+)?)s/)
    if (timeMatch) {
      const minutes = parseInt(timeMatch[1], 10)
      const seconds = parseFloat(timeMatch[2])
      return minutes * 60 + seconds
    }
    
    // åŒ¹é… "try again in 30s" æ ¼å¼
    const secondsMatch = message.match(/try again in (\d+(?:\.\d+)?)s/)
    if (secondsMatch) {
      return parseFloat(secondsMatch[1])
    }
    
    // é»˜è®¤ç­‰å¾…1åˆ†é’Ÿ
    return 60
  }

  /**
   * åˆ¤æ–­æ˜¯å¦åº”è¯¥é™çº§åˆ°Together AI
   */
  private static shouldFallbackToTogether(error: any): boolean {
    // é™æµé”™è¯¯
    if (this.isRateLimitError(error)) {
      return true
    }
    
    // æœåŠ¡ä¸å¯ç”¨é”™è¯¯
    if (error?.status >= 500) {
      return true
    }
    
    // APIå¯†é’¥æ— æ•ˆ
    if (error?.status === 401) {
      return true
    }
    
    // å…¶ä»–å¯æ¢å¤çš„é”™è¯¯
    return false
  }

  /**
   * å¤„ç†Together AIè½¬å½•ç»“æœ
   */
  private static processTogetherTranscriptionResult(transcription: any): TranscriptionResult {
    const result: TranscriptionResult = {
      text: transcription.text || '',
      language: transcription.language || 'unknown',
      confidence: 0.95, // Together AI Whisperé€šå¸¸æœ‰å¾ˆé«˜çš„å‡†ç¡®æ€§
      segments: []
    }

    // å¤„ç†åˆ†æ®µä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if (transcription.segments && Array.isArray(transcription.segments)) {
      result.segments = transcription.segments.map((segment: any) => ({
        start: segment.start || 0,
        end: segment.end || 0,
        text: segment.text || '',
        confidence: segment.confidence || 0.9
      }))
    }

    return result
  }



  /**
   * è·å–è½¬å½•ç»Ÿè®¡ä¿¡æ¯
   */
  static getTranscriptionStats(result: TranscriptionResult): {
    characterCount: number
    wordCount: number
    segmentCount: number
    averageConfidence: number
    estimatedDuration: number
  } {
    const characterCount = result.text.length
    const wordCount = result.text.split(/\s+/).filter(word => word.length > 0).length
    const segmentCount = result.segments?.length || 0
    
    let averageConfidence = result.confidence
    if (result.segments && result.segments.length > 0) {
      const totalConfidence = result.segments.reduce((sum, segment) => sum + segment.confidence, 0)
      averageConfidence = totalConfidence / result.segments.length
    }
    
    const estimatedDuration = result.segments && result.segments.length > 0
      ? Math.max(...result.segments.map(s => s.end))
      : wordCount / 150 * 60 // ä¼°ç®—ï¼š150è¯/åˆ†é’Ÿ
    
    return {
      characterCount,
      wordCount,
      segmentCount,
      averageConfidence: Math.round(averageConfidence * 1000) / 1000,
      estimatedDuration: Math.round(estimatedDuration)
    }
  }
}